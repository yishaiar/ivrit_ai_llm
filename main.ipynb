{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.dotenv import base_dir, data_dir\n",
    "from app.model.model import  Model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dotenv params: {'PARAM1_value'}\n",
      "base_dir: c:\\Users\\yili0901\\Downloads\\code\\ivrit_ai\\app\n",
      "data_dir: c:\\Users\\yili0901\\Downloads\\code\\ivrit_ai\\data\n",
      "model: loaded Model\n"
     ]
    }
   ],
   "source": [
    "print('dotenv params:', {os.getenv('PARAM1')})\n",
    "print('base_dir:', base_dir)\n",
    "print('data_dir:', data_dir)\n",
    "\n",
    "model = Model()\n",
    "print('model:', model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name1 = 'C:/Users/yili0901/Downloads/code/ivrit_ai/data/audio-transcripts'\n",
    "dataset_name2 = 'C:/Users/yili0901/Downloads/code/ivrit_ai/data/whisper-training'\n",
    "# dataset_name2 = dataset_name1\n",
    "r=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yili0901\\Downloads\\code\\ivrit_ai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 2183042\n",
      "})\n",
      "__________________________________________________\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 2183042\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict,concatenate_datasets\n",
    "\n",
    "# Load the first Huggingface dataset\n",
    "dataset1 = load_dataset(dataset_name1)\n",
    "# dataset1 = dataset1['train']  # Adjust according to your dataset split\n",
    "dataset1 = concatenate_datasets([dataset1['train']])\n",
    "\n",
    "print(dataset1)\n",
    "print('_'*50)\n",
    "# Load the second Huggingface dataset\n",
    "dataset2 = load_dataset(dataset_name2)\n",
    "# dataset2 = dataset2['train']  # Adjust according to your dataset split\n",
    "dataset2 = concatenate_datasets([dataset2['train'], dataset2['test']])\n",
    "# dataset2 = concatenate_datasets([dataset2['train']])\n",
    "\n",
    "print(dataset2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AnotherPodcastForStartups/2023.01.18 [מובילי חדשנות] כיצד הבינה המלאכותית תשנה את חיינו/325',\n",
       " 'Geekonomy/2016.03.14 גיקונומי #42 - אורי כץ/991',\n",
       " 'Geekonomy/2019.06.16 פרק #262 - אהרון אהרון ורשות החדשנות/0',\n",
       " 'Geekonomy/2020.06.07 פרק #334 - יגאל ליברנט והמורכבות האנושית/758',\n",
       " 'Geekonomy/2021.12.29 פרק #496 - אור דיין מדווש בעליה/924',\n",
       " 'Geekonomy/2022.08.02 פרק #589 - יואב לוי וסייבר בתעשיית הרכב/44',\n",
       " 'Geekonomy/2023.02.14 פרק #681 - הטור האישי של רענן שקד/44',\n",
       " 'SmallBigHistory/2019.03.24 הולכים לבחירות #3 - גולדה מאיר/320',\n",
       " 'SmallBigHistory/2020.06.10 #108 - דני אורבך יוצא להתנקש בהיטלר/334',\n",
       " 'Tziun3/2021.12.20 פרק #271 – מסכמים סיבוב בגביע/446'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "# Convert UUIDs to sets for comparison\n",
    "set_uuids1 = set(dataset1['uuid'])\n",
    "set_uuids2 = set(dataset2['uuid'])\n",
    "\n",
    "# Compare the UUIDs\n",
    "common_uuids = set_uuids1 & set_uuids2\n",
    "# unique_to_dataset1 = set_uuids1 - set_uuids2\n",
    "# unique_to_dataset2 = set_uuids2 - set_uuids1\n",
    "\n",
    "\n",
    "# sub sample\n",
    "common_uuids =set(list(common_uuids)[:10])\n",
    "common_uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary from uuid to the desired fields in dataset1\n",
    "fields_to_add = dataset1.column_names  # Adjust this list based on your requirements\n",
    "uuid_mapping1 = {row['uuid']: {field+'1': row[field] for field in fields_to_add} for row in dataset1 if row['uuid'] in common_uuids}\n",
    "type(uuid_mapping1)\n",
    "\n",
    "# Create a mapping dictionary from uuid to the desired fields in dataset1\n",
    "fields_to_add = dataset2.column_names  # Adjust this list based on your requirements\n",
    "uuid_mapping2 = {row['uuid']: {field+'2': row[field] for field in fields_to_add} for row in dataset2 if row['uuid'] in common_uuids}\n",
    "type(uuid_mapping2)\n",
    "\n",
    "\n",
    "# Define a function to update rows in dataset2 based on the uuid_mapping\n",
    "def update_row(uuid,row):\n",
    "    # uuid = row['uuid']\n",
    "    if uuid in uuid_mapping1:\n",
    "        for field, value in uuid_mapping1[uuid].items():\n",
    "            row[field] = value\n",
    "    return row\n",
    "updated_uuid_mapping = {uuid: update_row(uuid, row.copy()) for uuid, row in uuid_mapping2.items()}\n",
    "updated_dataset = Dataset.from_dict({key: [row[key] for row in updated_uuid_mapping.values()] for key in updated_uuid_mapping[list(updated_uuid_mapping.keys())[0]].keys()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['source2', 'episode2', 'uuid2', 'text2', 'attrs2', 'source1', 'episode1', 'uuid1', 'text1', 'attrs1'])\n",
      "dict_keys(['source1', 'episode1', 'uuid1', 'text1', 'attrs1'])\n",
      "dict_keys(['source2', 'episode2', 'uuid2', 'text2', 'attrs2'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "uuid = list(updated_uuid_mapping.keys())[0]\n",
    "print(updated_uuid_mapping[uuid].keys())\n",
    "print(uuid_mapping1[uuid].keys())\n",
    "print(uuid_mapping2[uuid].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source2', 'episode2', 'uuid2', 'text2', 'attrs2', 'source1',\n",
       "       'episode1', 'uuid1', 'text1', 'attrs1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(updated_dataset)\n",
    "\n",
    "# Display the DataFrame\n",
    "# print(df.head())\n",
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
