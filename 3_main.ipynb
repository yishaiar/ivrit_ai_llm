{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_dir: /home/yishai/Desktop/ivrit_ai_llm/app\n",
      "data_dir: /home/yishai/Desktop/ivrit_ai_llm/data\n",
      "golden_data_dir: /home/yishai/Desktop/ivrit_ai_llm/golden_data\n"
     ]
    }
   ],
   "source": [
    "from app.dotenv import base_dir, data_dir,golden_data_dir\n",
    "# from app.model.model import  Model\n",
    "# import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "# set the max columns to none\n",
    "pd.set_option('display.max_columns', None)\n",
    "# set the max columns to none\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "# print('dotenv params:', {os.getenv('PARAM1')})\n",
    "print('base_dir:', base_dir)\n",
    "print('data_dir:', data_dir)\n",
    "print('golden_data_dir:', golden_data_dir)\n",
    "\n",
    "# model = Model()\n",
    "# print('model:', model)\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg_logprob - the average log probability of the tokens in the transcription\n",
    "estimating the confidence of the transcription. Lower values (more negative) indicate lower confidence, while values closer to zero indicate higher confidence in the transcription.\n",
    "Values generally range between -1.0 and 0, below -0.5 is suspected below -1 is very suspected\n",
    "\n",
    "\n",
    "no_speech_prob - probability is high (close to 1), it suggests that the model believes the segment is likely silence or background noise, rather than actual speech. This can be useful for filtering out non-speech parts.\n",
    "Values range from 0 to 1, where 0 indicates a high likelihood that the segment contains speech, and 1 indicates a high probability that the segment contains no speech.\n",
    "\n",
    "compression_ratio - measure of how much the output text has been \"compressed\" compared to the input audio in terms of information content. Higher values might indicate potential issues, such as long stretches of repeated text or overly verbose output.\n",
    "Typically ranges from 1.0 to 2.5. A value of 1.0 means that the transcription is roughly equal in length to the original audio input, while higher values indicate more compression (shorter transcribed text relative to the audio duration).\n",
    "\n",
    "Suggested Workflow:\n",
    "\n",
    "Filter Low Confidence Segments: Identify segments with a low avg_logprob or a high no_speech_prob and prioritize these for refinement by the LLM.\n",
    "\n",
    "Fix Repetitive or Garbled Transcriptions: Use the compression_ratio to detect where the transcribed text might be problematic (too verbose, repeated words) and ask the LLM to clean up these sections.\n",
    "\n",
    "Time-based Post-Processing: Use the start and end timestamps to make sure the LLM processes segments in the right sequence, especially if you are breaking up the transcription for batch processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load df from file\n",
    "\n",
    "1. add wanted attribute columns & remove nulls  \n",
    "2. drop columns\n",
    "3. remove formatting chracters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: sentence, dtype: object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{golden_data_dir}/crowd-transcribe-v4-enriched.csv')\n",
    "ind = ~df['attrs'].isnull()\n",
    "df.loc[ind,'attrs'] = df.loc[ind,'attrs'].apply(lambda x: json.loads(x))\n",
    "df = df.loc[((~df['sentence'].isnull())*(~df['orig_sentence'].isnull()))]\n",
    "# verify nulls are dropped\n",
    "df['sentence'].loc[(~(~df['sentence'].isnull())*(~df['orig_sentence'].isnull()))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111858\n",
      "1901\n",
      "5307\n"
     ]
    }
   ],
   "source": [
    "vals = {'avg_logprob':-0.5,'no_speech_prob':0.5,'compression_ratio':2.0}\n",
    "inds = {}\n",
    "for key,value in vals.items():\n",
    "    df[key] = value\n",
    "    # df.loc[0,'attrs']['segments'][0]['avg_logprob']\n",
    "    df[key] = None\n",
    "    df.loc[ind,key] = df.loc[ind,'attrs'].apply( lambda x:  x['segments'][0][key] )\n",
    "    # data = df.loc[ind,key].loc[df.loc[ind,key]>value]\n",
    "    inds[key] = (df.loc[ind,key].loc[df.loc[ind,key]>value].index,df.loc[ind,key].loc[df.loc[ind,key]<value].index)    \n",
    "    print(len(inds[key][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['orig_sentence', 'sentence', 'source', 'episode', 'avg_logprob',\n",
       "       'no_speech_prob', 'compression_ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['attrs','is_retranscribe','uuid'],inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove chracters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars(df: DataFrame,columns,remove_ords =[],remove_chars =[],replace_char = '') -> DataFrame:\n",
    "\n",
    "    def replace_chars_from_col(col,in_chars,replace_char = '',is_ord = False):\n",
    "        for ch in in_chars:\n",
    "            if is_ord:\n",
    "                col = col.str.replace(chr(ch),replace_char)\n",
    "            else:\n",
    "                col = col.str.replace(ch,replace_char)\n",
    "            \n",
    "        # for ch in ['  ','-']:\n",
    "        #     col = col.str.replace(ch,' ')\n",
    "        return col\n",
    "\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = replace_chars_from_col(df[col].copy(),remove_ords,replace_char = replace_char,is_ord = True)\n",
    "\n",
    "        df[col] = replace_chars_from_col(df[col].copy(),remove_chars,replace_char = replace_char,is_ord = False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove error characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing chars and their ords\n",
      " ,\t!,\t\",\t$,\t%,\t&,\t',\t(,\t),\t+,\t,,\t-,\t.,\t0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\t:,\t=,\t?,\t[,\t],\ta,\tb,\tc,\td,\te,\tf,\tg,\th,\ti,\tj,\tk,\tl,\tm,\tn,\to,\tp,\tq,\tr,\ts,\tt,\tu,\tv,\tw,\tx,\ty,\tz,\tż,\tα,\tζ,\tμ,\tο,\tπ,\tа,\tб,\tв,\tг,\tд,\tе,\tи,\tк,\tл,\tо,\tп,\tр,\tс,\tу,\tш,\tь,\tא,\tב,\tג,\tד,\tה,\tו,\tז,\tח,\tט,\tי,\tך,\tכ,\tל,\tם,\tמ,\tן,\tנ,\tס,\tע,\tף,\tפ,\tץ,\tצ,\tק,\tר,\tש,\tת,\n",
      "32,\t33,\t34,\t36,\t37,\t38,\t39,\t40,\t41,\t43,\t44,\t45,\t46,\t48,\t49,\t50,\t51,\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t61,\t63,\t91,\t93,\t97,\t98,\t99,\t100,\t101,\t102,\t103,\t104,\t105,\t106,\t107,\t108,\t109,\t110,\t111,\t112,\t113,\t114,\t115,\t116,\t117,\t118,\t119,\t120,\t121,\t122,\t380,\t945,\t950,\t956,\t959,\t960,\t1072,\t1073,\t1074,\t1075,\t1076,\t1077,\t1080,\t1082,\t1083,\t1086,\t1087,\t1088,\t1089,\t1091,\t1096,\t1100,\t1488,\t1489,\t1490,\t1491,\t1492,\t1493,\t1494,\t1495,\t1496,\t1497,\t1498,\t1499,\t1500,\t1501,\t1502,\t1503,\t1504,\t1505,\t1506,\t1507,\t1508,\t1509,\t1510,\t1511,\t1512,\t1513,\t1514,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_existing_unique_chars(cols):\n",
    "    \n",
    "    def cols_unique_characters(cols,print_ord = False):\n",
    "        uniq_chars = sorted(set(''.join(''.join(col) for col in cols)))\n",
    "        if print_ord:\n",
    "            return [ord(ch) for ch in uniq_chars]\n",
    "        return uniq_chars\n",
    "    \n",
    "    uniq1 = cols_unique_characters(cols)\n",
    "    uniq2 = cols_unique_characters(cols,print_ord = True)\n",
    "    print ('existing chars and their ords') \n",
    "    print('\\t'.join([str(x)+',' for x in uniq1]))\n",
    "    print('\\t'.join([str(x)+',' for x in uniq2]))\n",
    "    \n",
    "\n",
    "# error characters:\n",
    "\n",
    "error_ords =[]\n",
    "error_ords1 = [1470,1468,1465,1463,1462,1460,1099,1464,47,1523,\t1524,\t1575,\t1576,\t1578,\t1582,\t1585,\t1603,\t1606,\t8211,\t8212,\t8230,\t9834,\t9835,\t20126,\t21478,\t24555,\t26126,\t26449,\t27794,\t29978,\t35201,\t38590,\t40636,\t46988,\t47673,\t49324,\t49900,\t50508,\t50520,\t50612,\t50880,\t51032,\t51060,\t65533,]\n",
    "error_ords += error_ords1\n",
    "# # latin letters - might be usefull to keep\n",
    "# remove_ords2 = [380,\t945,\t950,\t956,\t959,\t960,\t1072,\t1073,\t1074,\t1075,\t1076,\t1077,\t1080,\t1082,\t1083,\t1086,\t1087,\t1088,\t1089,\t1091,\t1096,\t1100]\n",
    "# remove_ords += remove_ords2\n",
    "\n",
    "error_chars = ['\\u202b','\\u200f','\\u200e','\\n']\n",
    "\n",
    "   \n",
    "df = remove_chars(df.copy(),columns = ['orig_sentence','sentence'],\n",
    "                  remove_ords = error_ords,remove_chars = error_chars,\n",
    "                  replace_char = '')\n",
    "\n",
    "df = remove_chars(df.copy(),columns = ['orig_sentence','sentence'],\n",
    "                  remove_chars = ['  '], replace_char = ' ')        \n",
    "print_existing_unique_chars([df['orig_sentence'].copy(),df['orig_sentence'].copy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove empty columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in ['orig_sentence','sentence']:\n",
    "    ind = df[col].str.len()==0\n",
    "    df = df.loc[~ind]   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save temporary copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline results - filter out samples without mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WER - word error rate\n",
    "\n",
    "calculate how well the hypothesis (orig_sentence) matches the correct reference (sentence)\n",
    "\n",
    "WER = (S+D+I)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jiwer import wer\n",
    "\n",
    "# reference = \"the correct translation\"\n",
    "# hypothesis = \"your corrected sentence\"\n",
    "\n",
    "# error_rate = wer(reference, hypothesis)\n",
    "# print(f\"Word Error Rate: {error_rate}\")\n",
    "import jiwer\n",
    "\n",
    "\n",
    "transforms = jiwer.Compose(\n",
    "    [\n",
    "        jiwer.ExpandCommonEnglishContractions(),\n",
    "        jiwer.RemoveEmptyStrings(),\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemoveMultipleSpaces(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.ReduceToListOfListOfWords(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "df['wer'] = df.apply(lambda x: jiwer.wer(x['sentence'],x['orig_sentence'],\n",
    "                                   truth_transform=transforms, hypothesis_transform=transforms,\n",
    "                                   )\n",
    "                     ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero wer: 78808  percentage: 0.50085479862978\n"
     ]
    }
   ],
   "source": [
    "LEN = len(df.loc[df['wer']==0.0])\n",
    "print('zero wer:',LEN,' percentage:',LEN/len(df))\n",
    "# zero wer: 78808  percentage: 0.50085479862978\n",
    "\n",
    "\n",
    "df = df.loc[df['wer']>0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pre processing - remove formatting:\n",
    "\n",
    "they are not errors - just to find mistake senetences\n",
    "\n",
    "later they should be added again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove formatting characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing chars and their ords\n",
      " ,\t0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\ta,\tb,\tc,\td,\te,\tf,\tg,\th,\ti,\tj,\tk,\tl,\tm,\tn,\to,\tp,\tq,\tr,\ts,\tt,\tu,\tv,\tw,\tx,\ty,\tz,\tż,\tα,\tζ,\tμ,\tο,\tπ,\tа,\tб,\tв,\tг,\tд,\tе,\tи,\tк,\tл,\tо,\tп,\tр,\tс,\tу,\tш,\tь,\tא,\tב,\tג,\tד,\tה,\tו,\tז,\tח,\tט,\tי,\tך,\tכ,\tל,\tם,\tמ,\tן,\tנ,\tס,\tע,\tף,\tפ,\tץ,\tצ,\tק,\tר,\tש,\tת,\n",
      "32,\t48,\t49,\t50,\t51,\t52,\t53,\t54,\t55,\t56,\t57,\t97,\t98,\t99,\t100,\t101,\t102,\t103,\t104,\t105,\t106,\t107,\t108,\t109,\t110,\t111,\t112,\t113,\t114,\t115,\t116,\t117,\t118,\t119,\t120,\t121,\t122,\t380,\t945,\t950,\t956,\t959,\t960,\t1072,\t1073,\t1074,\t1075,\t1076,\t1077,\t1080,\t1082,\t1083,\t1086,\t1087,\t1088,\t1089,\t1091,\t1096,\t1100,\t1488,\t1489,\t1490,\t1491,\t1492,\t1493,\t1494,\t1495,\t1496,\t1497,\t1498,\t1499,\t1500,\t1501,\t1502,\t1503,\t1504,\t1505,\t1506,\t1507,\t1508,\t1509,\t1510,\t1511,\t1512,\t1513,\t1514,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "formatting_ords = [32,\t33,\t34,\t36,\t37,\t38,\t39,\t40,\t41,\t43,\t44,\t45,\t46,58,\t61,\t63,\t91,\t93,]\n",
    "formatting_chars = 'ההה'\n",
    "df = remove_chars(df.copy(),columns = ['orig_sentence','sentence'],\n",
    "                  remove_ords = formatting_ords,\n",
    "                #   remove_chars = formatting_chars,    \n",
    "                  replace_char = ' ')    \n",
    "print_existing_unique_chars([df['orig_sentence'].copy(),df['orig_sentence'].copy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove starting charcters - not sure it matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove_chars =[' ']\n",
    "\n",
    "# # data = ['שלום מה שלומך',' שלום מה שלומך','שלום מה שלומך ','שלום מה שלומך']\n",
    "# # df1 = pd.DataFrame([s],columns = ['orig_sentence'])\n",
    "# # print(df1.values)\n",
    "\n",
    "# def remove_start_end_chars(df: DataFrame,columns,remove_chars =[],) -> DataFrame:\n",
    "#     def remove_start_end_chars_from_col(col,in_chars):\n",
    "#         for ch in in_chars:\n",
    "#             ind = col.str.startswith(ch)\n",
    "#             col.loc[ind] = col.loc[ind].str[1:]\n",
    "#             ind = col.str.endswith(ch)\n",
    "#             col.loc[ind] = col.loc[ind].str[:-1]\n",
    "            \n",
    "#             # col = col.str.replace('^'+ch,'')\n",
    "#             # col = col.str.replace(ch+'$','')\n",
    "#         return col\n",
    "#     for col in columns:\n",
    "#         df[col] = remove_start_end_chars_from_col(df[col].copy(),remove_chars)\n",
    "\n",
    "#     return df\n",
    "    \n",
    "\n",
    "# df = remove_start_end_chars(df.copy(),['orig_sentence','sentence'],remove_chars = ['     ','    ','   ','  ',' '])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove shouts and laughter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yam-peleg/Hebrew-Mistral-7B-200K\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"yam-peleg/Hebrew-Mistral-7B-200K\")\n",
    "\n",
    "text = \"שלום!מהשלמהיום?\"\n",
    "# input_ids = tokenizer(text, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "# print(input_ids)\n",
    "tokens = tokenizer(text )\n",
    "print(tokens['input_ids'])  \n",
    "decoded_text = tokenizer.decode(tokens['input_ids'])\n",
    "print(decoded_text)  \n",
    "\n",
    "encoded_text = tokenizer.convert_ids_to_tokens(tokens['input_ids'])\n",
    "\n",
    "# Print the tokens\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### analyisis\n",
    "\n",
    "1. seperate data into seperate words (remove formatting)\n",
    "2. find the hebrew words errors\n",
    "3. find the errors location in original df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seperate data into seperate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_numbers(uniq):\n",
    "    return [x for x in uniq if not x.isnumeric()]\n",
    "def df_unique_words(col):\n",
    "    joined = ' '.join(col)\n",
    "    uniq =  sorted(list(set(joined.split(' '))))\n",
    "    uniq = remove_numbers(uniq)\n",
    "    \n",
    "    \n",
    "    return uniq\n",
    "unique_words = df_unique_words(df['orig_sentence'].copy().tolist())\n",
    "# uniq = df_unique_words(uniq)\n",
    "# uniq = df_unique_words(uniq)\n",
    "# print(uniq)\n",
    "# uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def find_hebrew_words(uniq):\n",
    "    hebrew_words = [word for word in uniq if any(ord(ch)>1487 for ch in word)]\n",
    "    return hebrew_words\n",
    "\n",
    "def find_error_words(hebrew_words):\n",
    "    errors = []\n",
    "    end_letters = set([1509,1507,1503,1501])\n",
    "    errors += [word for word in hebrew_words if any(ord(ch) in end_letters for ch in word[:-1])]\n",
    "    \n",
    "    # english_letters = set(range(97,123))\n",
    "    # errors += [word for word in hebrew_words if any(ord(ch) in english_letters for ch in word)]\n",
    "    \n",
    "    \n",
    "    # todo: א vs ע ה\n",
    "    # todo: 'כ' vs 'ק'\n",
    "    # todo:\n",
    "    # todo: compare to dictionairy\n",
    "    # todo: words with foreign letters  - (might be translated fonetically)\n",
    "    # todo: words with numbers\n",
    "    # todo: words with special characters\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return errors\n",
    "hebrew_words = find_hebrew_words(unique_words)\n",
    "errors = find_error_words(hebrew_words)\n",
    "# errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'בלוםפילד': 3,\n",
       " 'יוםך': 2,\n",
       " 'כיףי': 2,\n",
       " 'מץט': 2,\n",
       " 'עםך': 2,\n",
       " 'קוץק': 2,\n",
       " 'בבלוםפילד': 1,\n",
       " 'בוואץמוק': 1,\n",
       " 'ביןך': 1,\n",
       " 'במוןиру': 1,\n",
       " 'דוםס': 1,\n",
       " 'היוםיומיים': 1,\n",
       " 'הקוץקר': 1,\n",
       " 'ותבציץו': 1,\n",
       " 'ותכיןי': 1,\n",
       " 'יעדיףו': 1,\n",
       " 'יקבץך': 1,\n",
       " 'ירוץו': 1,\n",
       " 'ישיםו': 1,\n",
       " 'כיףים': 1,\n",
       " 'כןהאמיתי': 1,\n",
       " 'כןים': 1,\n",
       " 'מיווןי': 1,\n",
       " 'מןבור': 1,\n",
       " 'סולםים': 1,\n",
       " 'עליהםו': 1,\n",
       " 'עםμοיח': 1,\n",
       " 'עםה': 1,\n",
       " 'עםו': 1,\n",
       " 'ערומיםך': 1,\n",
       " 'קץר': 1,\n",
       " 'רובןפינסקי': 1,\n",
       " 'רייליףמן': 1,\n",
       " 'שאילביץר': 1,\n",
       " 'שאיןבעלות': 1,\n",
       " 'שוטרוץקין': 1,\n",
       " 'תריםו': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_error_words_df_location(col,errors):\n",
    "    # errors_set = set(errors)\n",
    "    \n",
    "    error_location = {}\n",
    "    for error in errors:\n",
    "        ind = col.str.contains(error)\n",
    "        error_location[error] = df.loc[ind].index.tolist()\n",
    "    # todo: get the clean formatted sentence\n",
    "    # todo: get the location of the error in the sentence  \n",
    "    return error_location\n",
    "\n",
    "error_location = find_error_words_df_location(df['orig_sentence'].copy(),errors)\n",
    "{k: len(v) for k, v in sorted(error_location.items(), key=lambda item: len(item[1]),\n",
    "                              reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'בבלוםפילד': [135193],\n",
       " 'בוואץמוק': [69963],\n",
       " 'ביןך': [75561],\n",
       " 'בלוםפילד': [54165, 130521, 135193],\n",
       " 'במוןиру': [99266],\n",
       " 'דוםס': [116433],\n",
       " 'הקוץקר': [68705],\n",
       " 'ותבציץו': [37089],\n",
       " 'ותכיןי': [21437],\n",
       " 'יוםך': [64789, 66140],\n",
       " 'יקבץך': [33295],\n",
       " 'ירוץו': [8794],\n",
       " 'ישיםו': [93568],\n",
       " 'כיףי': [38441, 69633],\n",
       " 'כיףים': [69633],\n",
       " 'כןהאמיתי': [143138],\n",
       " 'כןים': [42977],\n",
       " 'מיווןי': [122720],\n",
       " 'מןבור': [63168],\n",
       " 'מץט': [92021, 115575],\n",
       " 'סולםים': [27921],\n",
       " 'עליהםו': [41291],\n",
       " 'עםμοיח': [69783],\n",
       " 'עםה': [68412],\n",
       " 'עםו': [137060],\n",
       " 'עםך': [98898, 131502],\n",
       " 'ערומיםך': [36278],\n",
       " 'קוץק': [68705, 110085],\n",
       " 'קץר': [54643],\n",
       " 'רובןפינסקי': [116508],\n",
       " 'רייליףמן': [62694],\n",
       " 'שאילביץר': [32572],\n",
       " 'שאיןבעלות': [54331],\n",
       " 'שוטרוץקין': [129034],\n",
       " 'תריםו': [118908]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['לגמרי כןים לגבי הכל', 'לגמרי כנים לגבי הכל', 'Geekonomy',\n",
       "       '2022.04.12 פרק #539 - נטע בונדי נהנית לקודד', -0.3959960862994194,\n",
       "       0.00720977783203125, 0.8780487804878049, np.float64(0.25)],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[42977].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dictabert MLM\n",
    "\n",
    "https://arxiv.org/abs/2308.16687"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_add = \"dicta-il/dictabert\"\n",
    "model_add = f\"{data_dir}\\\\dictabert\"\n",
    "print(model_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from torch import topk, tensor\n",
    "def process_sentence(sentence: str, current_word, new_word):\n",
    "    return sentence.replace(current_word, new_word), sentence.split(' ').index(current_word)+1\n",
    "\n",
    "\n",
    "def get_top_k_words(model, tokenizer, top_k: int = 3,sentence: str = '',word_location: int = 0):\n",
    "    output = model(tokenizer.encode(sentence, return_tensors='pt'))\n",
    "    # the [MASK] is the 7th token (including [CLS])\n",
    "    top_k = topk(output.logits[0, word_location, :], top_k)[1]\n",
    "    return tokenizer.convert_ids_to_tokens(top_k)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_add)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_add)\n",
    "model.eval()\n",
    "\n",
    "# sentence = 'בשנת 1948 השלים אפרים קישון את התמחותו בפיסול מתכת ובתולדות האמנות והחל לפרסם מאמרים הומוריסטיים'\n",
    "\n",
    "\n",
    "sentence = remove_special_chars(df['orig_sentence'].loc[135193])\n",
    "print(sentence)\n",
    "print('_'*150)\n",
    "current_word = 'בבלוםפילד'\n",
    "bert_input, word_location = process_sentence(sentence, current_word , new_word = '[MASK]')\n",
    "\n",
    "tokens = get_top_k_words(model, tokenizer, top_k = 3,sentence = bert_input,word_location = word_location)\n",
    "for token in tokens:\n",
    "    bert_output,_ = process_sentence(bert_input, current_word = '[MASK]', new_word = token)\n",
    "    print(bert_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'בשנת 1948 השלים אפרים קישון את \n",
    "התמחותו \n",
    "בפיסול מתכת ובתולדות האמנות והחל לפרסם מאמרים הומוריסטיים'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myam-peleg/Hebrew-Mistral-7B-200K\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_pretrained(\"yam-peleg/Hebrew-Mistral-7B-200K\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mשלום!מהשלמהיום?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/ivrit_ai_llm/.venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:897\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    895\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    896\u001b[0m         )\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m~/Desktop/ivrit_ai_llm/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2271\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2269\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2275\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2280\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ivrit_ai_llm/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2505\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2504\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2505\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2508\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2509\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2510\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/ivrit_ai_llm/.venv/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama_fast.py:157\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, legacy, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_prefix_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_slow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_bos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_eos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n",
      "File \u001b[0;32m~/Desktop/ivrit_ai_llm/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:132\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 132\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslow_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/ivrit_ai_llm/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:1597\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1590\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn instance of tokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be converted in a Fast tokenizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1591\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m No converter was found. Currently available slow->fast convertors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1592\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1593\u001b[0m     )\n\u001b[1;32m   1595\u001b[0m converter_class \u001b[38;5;241m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[0;32m-> 1597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ivrit_ai_llm/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:658\u001b[0m, in \u001b[0;36mSpmConverter.converted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[0;32m--> 658\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Tokenizer assemble\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     normalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto)\n",
      "File \u001b[0;32m~/Desktop/ivrit_ai_llm/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:578\u001b[0m, in \u001b[0;36mSpmConverter.tokenizer\u001b[0;34m(self, proto)\u001b[0m\n\u001b[1;32m    569\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(\n\u001b[1;32m    570\u001b[0m         Unigram(\n\u001b[1;32m    571\u001b[0m             vocab_scores,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[1;32m    575\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 578\u001b[0m     _, merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSpmExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     bpe_vocab \u001b[38;5;241m=\u001b[39m {word: i \u001b[38;5;28;01mfor\u001b[39;00m i, (word, score) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vocab_scores)}\n\u001b[1;32m    580\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(\n\u001b[1;32m    581\u001b[0m         BPE(\n\u001b[1;32m    582\u001b[0m             bpe_vocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m         )\n\u001b[1;32m    589\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/ivrit_ai_llm/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:95\u001b[0m, in \u001b[0;36mSentencePieceExtractor.extract\u001b[0;34m(self, vocab_scores)\u001b[0m\n\u001b[1;32m     92\u001b[0m sp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp\n\u001b[1;32m     93\u001b[0m vocab \u001b[38;5;241m=\u001b[39m {sp\u001b[38;5;241m.\u001b[39mid_to_piece(index): index \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sp\u001b[38;5;241m.\u001b[39mGetPieceSize())}\n\u001b[0;32m---> 95\u001b[0m merges \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_merges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vocab, merges\n",
      "File \u001b[0;32m~/Desktop/ivrit_ai_llm/.venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:70\u001b[0m, in \u001b[0;36mgenerate_merges\u001b[0;34m(vocab, vocab_scores)\u001b[0m\n\u001b[1;32m     67\u001b[0m     local \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(local, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: (vocab[x[\u001b[38;5;241m0\u001b[39m]], vocab[x[\u001b[38;5;241m1\u001b[39m]]))\n\u001b[1;32m     68\u001b[0m     merges\u001b[38;5;241m.\u001b[39mextend(local)\n\u001b[0;32m---> 70\u001b[0m merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(merges, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m val: (val[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mlen\u001b[39m(val[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(val[\u001b[38;5;241m1\u001b[39m])), reverse\u001b[38;5;241m=\u001b[39mreverse)\n\u001b[1;32m     71\u001b[0m merges \u001b[38;5;241m=\u001b[39m [(val[\u001b[38;5;241m0\u001b[39m], val[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m merges]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merges\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTScore is excellent for semantic similarity and will show you how much your corrections make the meaning closer to the reference.\n",
    "BLEU and ROUGE are good if you care about exact word matches (BLEU) or content recall (ROUGE).\n",
    "WER gives a direct error count, showing how many errors remain.\n",
    "Cosine Similarity between sentence embeddings provides a more holistic view of how semantically close your corrections are to the reference.\n",
    "If your corrections focus on improving the meaning and not just the wording, BERTScore or Cosine Similarity would likely be the best metrics. If you're aiming for exact match or word-level accuracy, go with BLEU, ROUGE, or WER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Reference translations (can have multiple references)\n",
    "references = [\n",
    "    ['the', 'cat', 'is', 'on', 'the', 'mat'],\n",
    "    ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']\n",
    "]\n",
    "\n",
    "# Candidate translation (generated by a machine)\n",
    "candidate = ['the', 'cat', 'is', 'on', 'the', 'mat']\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = sentence_bleu(references, candidate, smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sentences to compare\n",
    "ref_sentence = \"the correct translation\"\n",
    "corrected_sentence = \"your corrected sentence\"\n",
    "\n",
    "# Tokenize and get embeddings\n",
    "ref_tokens = tokenizer(ref_sentence, return_tensors=\"pt\")\n",
    "corrected_tokens = tokenizer(corrected_sentence, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    ref_embedding = model(**ref_tokens).last_hidden_state.mean(dim=1)\n",
    "    corrected_embedding = model(**corrected_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "similarity = cosine_similarity(ref_embedding, corrected_embedding).item()\n",
    "print(f\"Cosine Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{data_dir}\\\\dictabert\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(f\"{data_dir}\\\\dictabert\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "sentence = 'בשנת 1948 השלים אפרים קישון את [MASK] בפיסול מתכת ובתולדות האמנות והחל לפרסם מאמרים הומוריסטיים'\n",
    "\n",
    "output = model(tokenizer.encode(sentence, return_tensors='pt'))\n",
    "# the [MASK] is the 7th token (including [CLS])\n",
    "import torch\n",
    "top_2 = torch.topk(output.logits[0, 7, :], 2)[1]\n",
    "print('\\n'.join(tokenizer.convert_ids_to_tokens(top_2))) # should print מחקרו / התמחותו "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "from bert_score import BERTScorer,score\n",
    "\n",
    "# Example texts\n",
    "reference = \"This is a reference text example.\"\n",
    "candidate = \"This is a candidate text example.\"\n",
    "# BERTScore calculation\n",
    "# scorer = BERTScorer(model_type='')\n",
    "# P, R, F1 = scorer.score([candidate], [reference])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Load the Hebrew BERT model and tokenizer\n",
    "model_name = f\"{data_dir}\\\\bert-base-uncased\"\n",
    "# Define the reference and candidate sentences in Hebrew\n",
    "reference_sentences = ['הטקסט המדויק הזה הוא המושלם.']\n",
    "candidate_sentences = []'המשפט הזה הוא הטוב ביותר.']\n",
    "lang = 'he'\n",
    "\n",
    "\n",
    "# model_name = f\"{data_dir}\\\\bert-base-uncased\"\n",
    "# reference_sentence = \"This is a reference text example.\"\n",
    "# candidate_sentence = \"This is a candidate text example.\"\n",
    "lang = 'en'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# required when loaded from local instead of hugginface\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "num_layers = model.config.num_hidden_layers\n",
    "P, R, F1 = score(candidate_sentence, reference_sentence,model_type=model_name,num_layers = num_layers,rescale_with_baseline=True,lang='he')\n",
    "print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to get token embeddings for a list of sentences\n",
    "def get_token_embeddings(sentences,model,tokenizer):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the input sentences\n",
    "        inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "        # Get the embeddings\n",
    "        outputs = model(**inputs)\n",
    "        # Get the embeddings for all tokens\n",
    "        embeddings = outputs.last_hidden_state\n",
    "    return embeddings\n",
    "\n",
    "# Compute BERTScore Precision, Recall, and F1\n",
    "def compute_bert_score(reference, candidate,model_name):\n",
    "    print('model_name:', model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    ref_embeddings = get_token_embeddings([reference],model,tokenizer)\n",
    "    cand_embeddings = get_token_embeddings([candidate],model,tokenizer)\n",
    "    \n",
    "    # embeddings for a list of sentences (not by token)\n",
    "    similarity =  cosine_similarity(ref_embeddings.mean(dim=1),cand_embeddings.mean(dim=1))\n",
    "    BERTScore = similarity[0][0]\n",
    "    \n",
    "    # Compute cosine similarities between each token in the candidate and reference\n",
    "    ref_tokens = tokenizer(reference, return_tensors='pt')['input_ids'].squeeze().tolist()\n",
    "    cand_tokens = tokenizer(candidate, return_tensors='pt')['input_ids'].squeeze().tolist()\n",
    "    \n",
    "    ref_embeddings = ref_embeddings.squeeze(0)\n",
    "    cand_embeddings = cand_embeddings.squeeze(0)\n",
    "    \n",
    "    \n",
    "    # Token-level similarity matrix\n",
    "    similarity_matrix = cosine_similarity(cand_embeddings.numpy(), ref_embeddings.numpy())\n",
    "    \n",
    "    # Compute precision\n",
    "    precision_scores = []\n",
    "    for i, cand_token in enumerate(cand_tokens):\n",
    "        cand_emb = cand_embeddings[i].unsqueeze(0).numpy()\n",
    "        similarities = similarity_matrix[i]\n",
    "        max_similarity = max(similarities)\n",
    "        precision_scores.append(max_similarity)\n",
    "    \n",
    "    precision = np.mean(precision_scores)\n",
    "    \n",
    "    # Compute recall\n",
    "    recall_scores = []\n",
    "    for j, ref_token in enumerate(ref_tokens):\n",
    "        ref_emb = ref_embeddings[j].unsqueeze(0).numpy()\n",
    "        similarities = similarity_matrix[:, j]\n",
    "        max_similarity = max(similarities)\n",
    "        recall_scores.append(max_similarity)\n",
    "    \n",
    "    recall = np.mean(recall_scores)\n",
    "    \n",
    "    # F1 score as the harmonic mean of precision and recall\n",
    "    if precision + recall > 0:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    \n",
    "    return precision, recall, f1,BERTScore\n",
    "\n",
    "\n",
    "# Load the Hebrew BERT model and tokenizer\n",
    "model_name = f\"{data_dir}\\\\bert-base-uncased\"\n",
    "# Define the reference and candidate sentences in Hebrew\n",
    "reference_sentence = 'הטקסט המדויק הזה הוא המושלם.'\n",
    "candidate_sentence = 'המשפט הזה הוא הטוב ביותר.'\n",
    "\n",
    "# model_name = f\"{data_dir}\\\\bert-base-uncased\"\n",
    "# reference_sentence = \"This is a reference text example.\"\n",
    "# candidate_sentence = \"This is a candidate text example.\"\n",
    "\n",
    "\n",
    "# Compute BERTScore\n",
    "precision, recall, f1,BERTScore = compute_bert_score(reference_sentence, candidate_sentence,model_name)\n",
    "\n",
    "# Print BERTScore Precision, Recall, and F1\n",
    "print(f\"BERTScore Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, BERTScore: {BERTScore:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df1.copy()\n",
    "df.rename(columns={'orig_sentence':'x_orig', 'sentence':'y_orig'}, inplace=True)\n",
    "df.drop(['uuid','is_retranscribe'], axis=1, inplace=True)\n",
    "df = df.loc[:100]\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['x'] = remove_special_chars(df['x_orig'].copy())\n",
    "df['y'] = remove_special_chars(df['y_orig'].copy())\n",
    "\n",
    "# count words in df\n",
    "df['x_count'] = df['x'].apply(lambda x: len(str(x).split()))\n",
    "df['y_count'] = df['y'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "df[['x','y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_unique_words(df):\n",
    "    return sorted(list(set(' '.join(df).split())))\n",
    "print(df_unique_words(df['x']))\n",
    "# df['x'].str.split('\\s+', expand=True)\n",
    "#                   .stack()\n",
    "#                   .drop_duplicates()\n",
    "#                   .tolist()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df['x'].loc[8].split(' ')\n",
    "b = df['y'].loc[8].split(' ')\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arr = []\n",
    "aa = []\n",
    "bb=[]\n",
    "i,j = len(a)-1,len(b)-1\n",
    "while i>=0 and j>=0:    \n",
    "    if a[i] == b[j]:\n",
    "        aa.append(a[i])\n",
    "        bb.append(b[j])\n",
    "        i-=1\n",
    "        j-=1\n",
    "    elif a[i] in b:\n",
    "        aa.append(a[i])\n",
    "        i+=1\n",
    "    else:\n",
    "        aa.append(a[i])\n",
    "        bb.append(None)\n",
    "        bb.append(b[j])\n",
    "        aa.append(None)\n",
    "        \n",
    "        i-=1\n",
    "        j-=1\n",
    "        \n",
    "\n",
    "    \n",
    "    #     \n",
    "    #     bb.append(None)\n",
    "    #     j+=1   \n",
    "        \n",
    "#     # else:\n",
    "#     #     aa.append(a[i])\n",
    "#     #     bb.append(None)\n",
    "#     #     aa.append(a[i])\n",
    "#     #     bb.append(None)\n",
    "#     #     i+=1\n",
    "#     #     j+=1\n",
    "\n",
    "# for i,word in enumerate(a):\n",
    "#     print(word)\n",
    "#     print(i,np.where(b == word)[0])\n",
    "#     print('_'*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_col(df,col,extra_cols=10):\n",
    "    # Split the 'x' column into separate columns\n",
    "    split_df = df[col].str.split(r'\\s+', expand=True)\n",
    "    # Rename the columns\n",
    "    columns = [f'{i+1}{col}' if i+1>9 else f'0{i+1}{col}' for i  in range(split_df.shape[1]+extra_cols)]\n",
    "    split_df.columns = columns[:split_df.shape[1]]\n",
    "    \n",
    "    # add extra empty columns for allowing to move columns later\n",
    "    for col in  columns[split_df.shape[1]:]:\n",
    "        split_df[col] = None  # You could use `None` instead of `pd.NA` if you prefer\n",
    "    # Join the new columns back to the original DataFrame\n",
    "    df = df.join(split_df)\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = split_col(df.copy(),'x')\n",
    "df = split_col(df.copy(),'y')\n",
    "df = df[sorted(df.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_row(row,from_col,name='x'):\n",
    "    ind = row.index\n",
    "    cols = [i for i in ind[row.notnull()] if i.endswith(name)][from_col-1:-1]\n",
    "    last = int(cols[-1].replace(name,''))+1\n",
    "    cols += [f'{last}{name}' if last>9 else f'0{last}{name}']\n",
    "    row[cols[1:]] = row[cols[:-1]] \n",
    "    row[cols[0]] = None \n",
    "    \n",
    "    \n",
    "    # row = row.append(pd.Series([None]*(len(row),len(from_col)-len(row)), index=from_col)\n",
    "    return row\n",
    "    \n",
    "# move_row(row=df.loc[0],from_col=1,to_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[0])\n",
    "print(move_row(row=df.loc[0].copy(),from_col=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_word(word):\n",
    "    # Decode the Hebrew words - drop the first 1488 unicode value of aleph letter\n",
    "    return [ord(char)-1488 for char in word]\n",
    "def decode_pair(word1,word2):\n",
    "    vector1 = decode_word(word1)  # Example vector for \"example\"\n",
    "    vector2 = decode_word(word2)  # Example vector for \"exemplary\"\n",
    "    LEN = max(len(vector1),len(vector2))\n",
    "    vector1 += [0]*(LEN-len(vector1))\n",
    "    vector2 += [0]*(LEN-len(vector2))\n",
    "    print(vector1),print(vector2)\n",
    "    correlation = pearsonr(vector1, vector2)[0]\n",
    "    print(f\"Pearson correlation coefficient: {correlation}\")\n",
    "    return vector1,vector2\n",
    "vector1,vector2 = decode_pair('שלום','בא')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "# Example phonetic feature vectors\n",
    "# In practice, these would be derived from a more complex phonetic analysis\n",
    "# vector1 = decode_word('הקבלה')  # Example vector for \"example\"\n",
    "# vector2 = decode_word('הקבלה')  # Example vector for \"exemplary\"\n",
    "\n",
    "# Compute Pearson correlation coefficient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count number of words in each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df['x'].str.replace(r'''[\\-\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\_\\+\\[\\]\\;\\'\\.\\,\\/\\{\\}\\:\\\"\\<\\>\\?\\|]''','')\n",
    "                #   .replace(r'[\\  \\   \\]',' ')\n",
    "                  .str\n",
    "                  .lower()\n",
    "                  .str\n",
    "                  .split('\\s+', expand=True)\n",
    "                  .stack()\n",
    "                  .drop_duplicates()\n",
    "                  .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df['y_count'] = df['y'].apply(lambda x: len(str(x).split()))\n",
    "remove [',','  '] in x\n",
    "for  \n",
    "df['x'] = df['x'].apply(lambda x: x.replace(',',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
