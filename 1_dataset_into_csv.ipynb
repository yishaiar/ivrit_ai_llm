{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir: /home/yishai/Desktop/ivrit_ai_llm/data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from app.dotenv import base_dir, data_dir\n",
    "# from app.model.model import  Model\n",
    "# import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from pandas import DataFrame\n",
    "from datasets import load_dataset, Dataset, DatasetDict,concatenate_datasets\n",
    "# import numpy as np\n",
    "# # set the max columns to none\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# # set the max columns to none\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# print('dotenv params:', {os.getenv('PARAM1')})\n",
    "# print('base_dir:', base_dir)\n",
    "print('data_dir:', data_dir)\n",
    "\n",
    "# # model = Model()\n",
    "# # print('model:', model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### datasets\n",
    "\n",
    "I verified all whisper-training are in crowd-transcribe-v4\n",
    "\n",
    "both have duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset1:, f/home/yishai/Desktop/ivrit_ai_llm/data/audio-transcripts, \n",
      "dict_keys(['train'])\n",
      "features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "num_rows: 2183042\n",
      "\n",
      "dataset2:, f/home/yishai/Desktop/ivrit_ai_llm/data/whisper-training, \n",
      "dict_keys(['train', 'test'])\n",
      "features: ['uuid', 'audio', 'orig_text', 'text'],\n",
      "num_rows: 63674\n",
      "\n",
      "dataset3:, f/home/yishai/Desktop/ivrit_ai_llm/data/crowd-transcribe-v4, \n",
      "dict_keys(['train', 'test'])\n",
      "features: ['uuid', 'audio', 'orig_sentence', 'sentence', 'is_retranscribe'],\n",
      "num_rows: 169376\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "selected dataset: /home/yishai/Desktop/ivrit_ai_llm/data/audio-transcripts\n"
     ]
    }
   ],
   "source": [
    "path_dataset1 = f'{data_dir}/audio-transcripts'\n",
    "info_dataset1 =  f'''\n",
    "dict_keys(['train'])\n",
    "features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
    "num_rows: 2183042\n",
    "'''\n",
    "\n",
    "path_dataset2 = f'{data_dir}/whisper-training'\n",
    "info_dataset2 =  f'''\n",
    "dict_keys(['train', 'test'])\n",
    "features: ['uuid', 'audio', 'orig_text', 'text'],\n",
    "num_rows: 63674\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path_dataset3 = f'{data_dir}/crowd-transcribe-v4'\n",
    "info_dataset3 =  f'''\n",
    "dict_keys(['train', 'test'])\n",
    "features: ['uuid', 'audio', 'orig_sentence', 'sentence', 'is_retranscribe'],\n",
    "num_rows: 169376\n",
    "'''\n",
    "# print('dataset3:', path_dataset3)\n",
    "\n",
    "\n",
    "print(f'dataset1:, f{path_dataset1}, {info_dataset1}')\n",
    "print(f'dataset2:, f{path_dataset2}, {info_dataset2}')\n",
    "print(f'dataset3:, f{path_dataset3}, {info_dataset3}')\n",
    "\n",
    "print('-'*100)\n",
    "\n",
    "path_dataset = path_dataset1\n",
    "\n",
    "print('selected dataset:', path_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load dataset into df (drop audio if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yishai/Desktop/ivrit_ai_llm/data/audio-transcripts\n",
      "dict_keys(['train'])\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 2183042\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_hf_dataset(path_dataset,cols_to_remove = 'audio'):\n",
    "    # Load the Huggingface dataset\n",
    "    dataset = load_dataset(path_dataset)\n",
    "    # dataset1 = dataset1['train']  \n",
    "    print(path_dataset)\n",
    "    print(dataset.keys())\n",
    "    # Adjust according to your dataset split\n",
    "    dataset = concatenate_datasets([dataset[key] for key in dataset.keys()])\n",
    "    \n",
    "    if cols_to_remove in dataset.features:\n",
    "        dataset = dataset.remove_columns(cols_to_remove)\n",
    "    print(dataset)\n",
    "    return dataset\n",
    "\n",
    "dataset = load_hf_dataset(path_dataset)\n",
    "uuid = \"929/2018.10.11 פרופ' נועם מזרחי/156\"\n",
    "results = [entry for entry in dataset if uuid in entry['uuid']]\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save to csv and verify file saved correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_into_df(chunk, path_dataset: str):\n",
    "    df = pd.DataFrame(chunk)\n",
    "    print(f'df.columns:{df.columns}')\n",
    "    \n",
    "    \n",
    "    # replace '' with None for corect csv export\n",
    "    df = df.replace('', None)\n",
    "    if 'attrs' in df.columns:\n",
    "        df['attrs'] = df['attrs'].apply(lambda x: json.dumps(x))\n",
    "\n",
    "    df.to_csv(f'{path_dataset}.csv', index=False)\n",
    "    df1 = pd.read_csv(f'{path_dataset}.csv')\n",
    "\n",
    "    ddf = df[~(df.isnull().sum(axis=1).astype(bool))]\n",
    "    ddf1 = df1[~(df1.isnull().sum(axis=1).astype(bool))]\n",
    "    if (ddf1==ddf).all().all():\n",
    "        print('df and df1 are equal')\n",
    "        del df,df1,ddf,ddf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "for dataset>100000: save a subset (chunk) of dataset and continue with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yishai/Desktop/ivrit_ai_llm/data/audio-transcripts\n",
      "dict_keys(['train'])\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 2183042\n",
      "})\n",
      "11\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "after chunking:\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 198459\n",
      "})\n",
      "df.columns:Index(['source', 'episode', 'uuid', 'text', 'attrs'], dtype='object')\n",
      "df and df1 are equal\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "after chunking:\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 198459\n",
      "})\n",
      "df.columns:Index(['source', 'episode', 'uuid', 'text', 'attrs'], dtype='object')\n",
      "df and df1 are equal\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "after chunking:\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 198459\n",
      "})\n",
      "df.columns:Index(['source', 'episode', 'uuid', 'text', 'attrs'], dtype='object')\n",
      "df and df1 are equal\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "after chunking:\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 198459\n",
      "})\n",
      "df.columns:Index(['source', 'episode', 'uuid', 'text', 'attrs'], dtype='object')\n",
      "df and df1 are equal\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "after chunking:\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 198458\n",
      "})\n",
      "df.columns:Index(['source', 'episode', 'uuid', 'text', 'attrs'], dtype='object')\n",
      "df and df1 are equal\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "after chunking:\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 198458\n",
      "})\n",
      "df.columns:Index(['source', 'episode', 'uuid', 'text', 'attrs'], dtype='object')\n",
      "df and df1 are equal\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "after chunking:\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 198458\n",
      "})\n",
      "df.columns:Index(['source', 'episode', 'uuid', 'text', 'attrs'], dtype='object')\n",
      "df and df1 are equal\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "after chunking:\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 198458\n",
      "})\n",
      "df.columns:Index(['source', 'episode', 'uuid', 'text', 'attrs'], dtype='object')\n",
      "df and df1 are equal\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "after chunking:\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 198458\n",
      "})\n",
      "df.columns:Index(['source', 'episode', 'uuid', 'text', 'attrs'], dtype='object')\n",
      "df and df1 are equal\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "after chunking:\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 198458\n",
      "})\n",
      "df.columns:Index(['source', 'episode', 'uuid', 'text', 'attrs'], dtype='object')\n",
      "df and df1 are equal\n",
      "______________________________________________________________________________________________________________________________________________________\n",
      "after chunking:\n",
      "Dataset({\n",
      "    features: ['source', 'episode', 'uuid', 'text', 'attrs'],\n",
      "    num_rows: 198458\n",
      "})\n",
      "df.columns:Index(['source', 'episode', 'uuid', 'text', 'attrs'], dtype='object')\n",
      "df and df1 are equal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_dataset_from_path(path_dataset: str,cols_to_remove = 'audio') -> DataFrame:\n",
    "    dataset = load_hf_dataset(path_dataset,cols_to_remove)\n",
    "    # # Load the Huggingface dataset\n",
    "    # dataset = load_dataset(path_dataset)\n",
    "    # # dataset1 = dataset1['train']  \n",
    "    # print(path_dataset)\n",
    "    # print(dataset.keys())\n",
    "    # # Adjust according to your dataset split\n",
    "    # dataset = concatenate_datasets([dataset[key] for key in dataset.keys()])\n",
    "    # print(dataset)\n",
    "    # ------------------------------------------------\n",
    "    # chunk to chunks if needed\n",
    "    num_shards = 1 + dataset.num_rows//200000\n",
    "    print(num_shards)\n",
    "    for i in range(num_shards):\n",
    "        chunk = dataset.shard(num_shards=num_shards, index=i)\n",
    "        # print(chunk)\n",
    "        # chunk.save_to_disk(f'{path_dataset}_chunk{i}')\n",
    "        # path_dataset = f'{path_dataset}_chunk{i}'\n",
    "        print('_'*150)\n",
    "        print('after chunking:')\n",
    "        \n",
    "        \n",
    "        print(chunk)\n",
    "        # remove dataset column\n",
    "        chunk_path = f'{path_dataset}_chunk{i}' if num_shards > 1 else path_dataset\n",
    "        save_into_df(chunk, chunk_path)\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "load_dataset_from_path(path_dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare whisper-training and crowd-transcribe-v4 \n",
    "\n",
    "check if there are any missing values in crowd-transcribe-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df):157405\n",
      "len(df):62662\n",
      "57941 57941\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicate_uuid(df: pd.DataFrame, col: str = 'uuid') -> pd.DataFrame:\n",
    "    # find the duplicte ocurence in the df1  - drop all except first occurence\n",
    "    df.drop(df.loc[df[col].duplicated()].index, inplace=True)\n",
    "    # print(len(df.loc[df[col].duplicated(keep=False)].index))\n",
    "    print(f'len(df):{len(df)}')\n",
    "    return df\n",
    "\n",
    "def get_common_values(df1, df2, col1, col2):\n",
    "    df1_common_values = df1[col1].isin(df2[col2])#true\\false\n",
    "    df1_common_index = df1.index[df1_common_values]#row index\n",
    "    df1_values = df1[col1].loc[df1_common_index]#the actual value\n",
    "    return df1_values\n",
    "\n",
    "df2 = pd.read_csv(f'{data_dir}/whisper-training.csv').reset_index(drop=True)\n",
    "df1 = pd.read_csv(f'{data_dir}/crowd-transcribe-v4.csv').reset_index(drop=True)\n",
    "df1 = remove_duplicate_uuid(df1)\n",
    "df2 = remove_duplicate_uuid(df2)\n",
    "col1 = 'uuid'\n",
    "col2 = 'uuid'\n",
    "\n",
    "df1_values = get_common_values(df1, df2, col1, col2)\n",
    "df2_values = get_common_values(df2, df1, col2, col1)\n",
    "\n",
    "print(len(df1_values), len(df2_values)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add metadata from audio-transcripts into crowd-transcribe-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(df):157405\n",
      "0\n",
      "len(df):198459\n",
      "uuids are equal: True\n",
      "1\n",
      "len(df):198459\n",
      "uuids are equal: True\n",
      "2\n",
      "len(df):198459\n",
      "uuids are equal: True\n",
      "3\n",
      "len(df):198459\n",
      "uuids are equal: True\n",
      "4\n",
      "len(df):198458\n",
      "uuids are equal: True\n",
      "5\n",
      "len(df):198458\n",
      "uuids are equal: True\n",
      "6\n",
      "len(df):198458\n",
      "uuids are equal: True\n",
      "7\n",
      "len(df):198458\n",
      "uuids are equal: True\n",
      "8\n",
      "len(df):198458\n",
      "uuids are equal: True\n",
      "9\n",
      "len(df):198458\n",
      "uuids are equal: True\n",
      "10\n",
      "len(df):198458\n",
      "uuids are equal: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('df1_values')\n",
    "df1 = pd.read_csv(f'{data_dir}/crowd-transcribe-v4.csv').reset_index(drop=True)\n",
    "df1 = remove_duplicate_uuid(df1)\n",
    "cols = ['attrs','source','episode']\n",
    "for col in cols:#cols to copy\n",
    "    df1[col]  = None\n",
    "print('_'*150)   \n",
    "for chunk_i in range(11):\n",
    "    print(chunk_i)\n",
    "    df2 = pd.read_csv(f'{data_dir}/audio-transcripts_chunk{chunk_i}.csv').reset_index(drop=True)\n",
    "    df2 = remove_duplicate_uuid(df2)\n",
    "    \n",
    "    df1_values = get_common_values(df1, df2, col1, col2)\n",
    "    df2_values = get_common_values(df2, df1, col2, col1)\n",
    "    df1_values.sort_values(inplace=True)\n",
    "    df2_values.sort_values(inplace=True)\n",
    "    \n",
    "    print('uuids are equal:', (df1.loc[df1_values.index, 'uuid'] == df2.loc[df2_values.index, 'uuid'].values).all())\n",
    "\n",
    "    for col in cols:#cols to copy\n",
    "        df1.loc[df1_values.index, col] = df2.loc[df2_values.index, col].values\n",
    "    print('_'*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save final result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df and df1 are equal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df1.copy()\n",
    "df.to_csv(f'{data_dir}/crowd-transcribe-v4-enriched.csv', index=False)\n",
    "df1 = pd.read_csv(f'{data_dir}/crowd-transcribe-v4-enriched.csv')\n",
    "\n",
    "ddf = df[~(df.isnull().sum(axis=1).astype(bool))]\n",
    "ddf1 = df1[~(df1.isnull().sum(axis=1).astype(bool))]\n",
    "if (ddf1==ddf).all().all():\n",
    "    print('df and df1 are equal')\n",
    "    del df1,ddf,ddf1#df,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 39317 mising uuid's from audio-transcripts sp not all rows with atributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uuid                   0\n",
       "orig_sentence          1\n",
       "sentence              52\n",
       "is_retranscribe        0\n",
       "attrs              39317\n",
       "source             39317\n",
       "episode            39317\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n",
    "\n",
    "# ind = df.loc[df['attrs'].isnull()].index\n",
    "\n",
    "# df['uuid'].loc[ind[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
